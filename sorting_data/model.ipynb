{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\No_sync\\analyst_ratings_processed_filtered.csv')\n",
    "sentences = df['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1889823744 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dinus\\OneDrive - Norwegian University of Life Sciences\\eik\\HackathonNMBU\\hackathon_NBIM_eiklab\\sorting_data\\model.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/eik/HackathonNMBU/hackathon_NBIM_eiklab/sorting_data/model.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/eik/HackathonNMBU/hackathon_NBIM_eiklab/sorting_data/model.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(sentences, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/eik/HackathonNMBU/hackathon_NBIM_eiklab/sorting_data/model.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m outputs \u001b[39m=\u001b[39m finbert(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/eik/HackathonNMBU/hackathon_NBIM_eiklab/sorting_data/model.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Define a softmax function\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dinus/OneDrive%20-%20Norwegian%20University%20of%20Life%20Sciences/eik/HackathonNMBU/hackathon_NBIM_eiklab/sorting_data/model.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:309\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 309\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    311\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    313\u001b[0m use_cache \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dinus\\anaconda3\\envs\\IND320\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1889823744 bytes."
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "outputs = finbert(**inputs)[0]\n",
    "\n",
    "# Define a softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Define labels for reference\n",
    "labels = {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your existing code for sentences, inputs, outputs, softmax, and labels\n",
    "\n",
    "# Create an empty list to store data\n",
    "data = []\n",
    "\n",
    "for idx, sent in enumerate(sentences):\n",
    "    probabilities = softmax(outputs.detach().numpy()[idx])\n",
    "    row_data = [sent] + [f'{prob * 100:.2f}%' for prob in probabilities]\n",
    "    data.append(row_data)\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "columns = ['title'] + [f'{label} Probability' for label in labels.values()]\n",
    "\n",
    "# Create a DataFrame\n",
    "new_df = pd.DataFrame(data, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(new_df, on='title', how='left')\n",
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>neutral Probability</th>\n",
       "      <th>positive Probability</th>\n",
       "      <th>negative Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7120.0</td>\n",
       "      <td>Tech Stocks And FAANGS Strong Again To Start D...</td>\n",
       "      <td>2020-06-10 11:33:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7121.0</td>\n",
       "      <td>10 Biggest Price Target Changes For Wednesday</td>\n",
       "      <td>2020-06-10 08:14:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7122.0</td>\n",
       "      <td>Benzinga Pro's Top 5 Stocks To Watch For Wed.,...</td>\n",
       "      <td>2020-06-10 07:53:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7123.0</td>\n",
       "      <td>Deutsche Bank Maintains Buy on Apple, Raises P...</td>\n",
       "      <td>2020-06-10 07:19:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7124.0</td>\n",
       "      <td>Apple To Let Users Trade In Their Mac Computer...</td>\n",
       "      <td>2020-06-10 06:27:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7125.0</td>\n",
       "      <td>Big Tech Reaches New Record Heights At The Sto...</td>\n",
       "      <td>2020-06-10 00:52:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>99.98%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7126.0</td>\n",
       "      <td>Why Apple's Stock Is Trading Higher Today</td>\n",
       "      <td>2020-06-09 15:14:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7127.0</td>\n",
       "      <td>Apple Could Announce In-House Chips For Macs A...</td>\n",
       "      <td>2020-06-09 13:58:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7128.0</td>\n",
       "      <td>Apple shares are trading higher despite market...</td>\n",
       "      <td>2020-06-09 12:41:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7129.0</td>\n",
       "      <td>Sonos Shares Spike To Session High, Now Up 9.5...</td>\n",
       "      <td>2020-06-09 11:11:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>91.16%</td>\n",
       "      <td>0.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7130.0</td>\n",
       "      <td>'iPhone 12 Production Expected to Begin in Jul...</td>\n",
       "      <td>2020-06-09 10:59:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.97%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7131.0</td>\n",
       "      <td>Cramer: NASDAQ Rebound Makes Sense As It Bette...</td>\n",
       "      <td>2020-06-09 10:31:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7132.0</td>\n",
       "      <td>Google Maps To Offer Relevant Local COVID-19 I...</td>\n",
       "      <td>2020-06-09 03:51:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.97%</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7133.0</td>\n",
       "      <td>IBM Discontinues Facial Recognition Technology...</td>\n",
       "      <td>2020-06-09 00:25:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>90.36%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>9.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7134.0</td>\n",
       "      <td>Munster Says Under-The-Radar Apple TV+ Will Co...</td>\n",
       "      <td>2020-06-08 14:16:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>87.63%</td>\n",
       "      <td>12.35%</td>\n",
       "      <td>0.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7135.0</td>\n",
       "      <td>Airlines Soar Again In Pre-Market Trading Amid...</td>\n",
       "      <td>2020-06-08 09:35:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7136.0</td>\n",
       "      <td>Apple Gets Patent For Socially-Distanced Group...</td>\n",
       "      <td>2020-06-08 04:34:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>91.74%</td>\n",
       "      <td>8.06%</td>\n",
       "      <td>0.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7137.0</td>\n",
       "      <td>Apple's Top Supplier Foxconn Launched New Recr...</td>\n",
       "      <td>2020-06-07 17:03:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>98.84%</td>\n",
       "      <td>1.15%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7138.0</td>\n",
       "      <td>Apple Preparing Monthly iPad, Mac Payment Plan...</td>\n",
       "      <td>2020-06-06 18:36:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7139.0</td>\n",
       "      <td>ESPAÑOL • Cannabis en Argentina, Blockchain Ur...</td>\n",
       "      <td>2020-06-05 17:19:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7140.0</td>\n",
       "      <td>PreMarket Prep Stock Of The Day: Genius Brands</td>\n",
       "      <td>2020-06-05 12:24:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7141.0</td>\n",
       "      <td>Airlines, Casino Stocks Getting A Lift In Pre-...</td>\n",
       "      <td>2020-06-05 10:43:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>99.98%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7142.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.94%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7143.0</td>\n",
       "      <td>No Surprise: Guess What Market Is Leading Post...</td>\n",
       "      <td>2020-06-05 10:08:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>33.81%</td>\n",
       "      <td>7.34%</td>\n",
       "      <td>58.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7144.0</td>\n",
       "      <td>Wedbush Maintains Outperform on Apple, Raises ...</td>\n",
       "      <td>2020-06-05 08:43:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7145.0</td>\n",
       "      <td>Credit Suisse Maintains Neutral on Apple, Rais...</td>\n",
       "      <td>2020-06-05 08:27:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>73.70%</td>\n",
       "      <td>14.65%</td>\n",
       "      <td>11.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7146.0</td>\n",
       "      <td>FaZe Clan Celebrates 10-Year Anniversary With ...</td>\n",
       "      <td>2020-06-05 07:02:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>98.92%</td>\n",
       "      <td>1.08%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7147.0</td>\n",
       "      <td>US Debt Market Raised $22.5B This Week, The Hi...</td>\n",
       "      <td>2020-06-05 05:42:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7148.0</td>\n",
       "      <td>Apple $425 Per Share Case 'On The Horizon,' An...</td>\n",
       "      <td>2020-06-05 00:07:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7149.0</td>\n",
       "      <td>FAANG Stocks Seem Immune To COVID-19</td>\n",
       "      <td>2020-06-04 13:47:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>90.14%</td>\n",
       "      <td>8.76%</td>\n",
       "      <td>1.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7150.0</td>\n",
       "      <td>'Apple's #iPhone to join Beijing's 12.2 billio...</td>\n",
       "      <td>2020-06-04 10:12:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7151.0</td>\n",
       "      <td>Why Genius Brands Is On A Massive Rally, Addin...</td>\n",
       "      <td>2020-06-04 01:01:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.48%</td>\n",
       "      <td>93.46%</td>\n",
       "      <td>0.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7152.0</td>\n",
       "      <td>Warner Music CEO On IPO: Music Is The 'Only Gl...</td>\n",
       "      <td>2020-06-03 15:47:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7153.0</td>\n",
       "      <td>Morgan Stanley Bullish On Rising App Store Rev...</td>\n",
       "      <td>2020-06-03 11:00:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7154.0</td>\n",
       "      <td>A Look Into Apple's Price Over Earnings</td>\n",
       "      <td>2020-06-03 10:47:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7155.0</td>\n",
       "      <td>Zoom Initially Slips Despite Strong Earnings, ...</td>\n",
       "      <td>2020-06-03 10:00:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>99.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7156.0</td>\n",
       "      <td>Morgan Stanley Maintains Overweight on Apple, ...</td>\n",
       "      <td>2020-06-03 08:31:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>7157.0</td>\n",
       "      <td>Warner Music Set To Go Public Today, In Antici...</td>\n",
       "      <td>2020-06-03 06:09:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7158.0</td>\n",
       "      <td>Peloton Makes Its Fitness App Available On App...</td>\n",
       "      <td>2020-06-03 04:57:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7159.0</td>\n",
       "      <td>'Apple is tracking iPhones stolen by looters' ...</td>\n",
       "      <td>2020-06-02 11:37:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>79.71%</td>\n",
       "      <td>0.47%</td>\n",
       "      <td>19.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7160.0</td>\n",
       "      <td>MoneyGram Shares Jump 50% As Western Union Rep...</td>\n",
       "      <td>2020-06-02 07:44:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.46%</td>\n",
       "      <td>99.54%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7161.0</td>\n",
       "      <td>Hearing Susquehanna Check Suggests Total iPhon...</td>\n",
       "      <td>2020-06-02 07:41:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7162.0</td>\n",
       "      <td>Tesla CEO Musk Says Other Three Officers Shoul...</td>\n",
       "      <td>2020-06-02 06:12:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7163.0</td>\n",
       "      <td>A Peek Into The Markets: US Stock Futures Up; ...</td>\n",
       "      <td>2020-06-02 06:03:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.33%</td>\n",
       "      <td>93.48%</td>\n",
       "      <td>0.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7164.0</td>\n",
       "      <td>Pepper Spray, Books On Racism, 'I Can't Breath...</td>\n",
       "      <td>2020-06-02 04:18:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>96.11%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>3.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7165.0</td>\n",
       "      <td>Apple Cuts iPhone Prices in China To Push Sale...</td>\n",
       "      <td>2020-06-02 00:35:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>0.57%</td>\n",
       "      <td>99.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7166.0</td>\n",
       "      <td>Gun, Security Stocks Trade Higher After Weeken...</td>\n",
       "      <td>2020-06-01 10:14:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.81%</td>\n",
       "      <td>3.96%</td>\n",
       "      <td>88.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7167.0</td>\n",
       "      <td>Carter Worth And Mike Khouw's Apple Trade</td>\n",
       "      <td>2020-06-01 08:20:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>7168.0</td>\n",
       "      <td>Apple Pays Hacker From India $100,000 For Disc...</td>\n",
       "      <td>2020-06-01 06:53:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>99.91%</td>\n",
       "      <td>0.01%</td>\n",
       "      <td>0.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>7169.0</td>\n",
       "      <td>Volume Production of New iPhone Models to Begi...</td>\n",
       "      <td>2020-06-01 05:39:00-04:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>100.00%</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0       7120.0  Tech Stocks And FAANGS Strong Again To Start D...   \n",
       "1       7121.0      10 Biggest Price Target Changes For Wednesday   \n",
       "2       7122.0  Benzinga Pro's Top 5 Stocks To Watch For Wed.,...   \n",
       "3       7123.0  Deutsche Bank Maintains Buy on Apple, Raises P...   \n",
       "4       7124.0  Apple To Let Users Trade In Their Mac Computer...   \n",
       "5       7125.0  Big Tech Reaches New Record Heights At The Sto...   \n",
       "6       7126.0          Why Apple's Stock Is Trading Higher Today   \n",
       "7       7127.0  Apple Could Announce In-House Chips For Macs A...   \n",
       "8       7128.0  Apple shares are trading higher despite market...   \n",
       "9       7129.0  Sonos Shares Spike To Session High, Now Up 9.5...   \n",
       "10      7130.0  'iPhone 12 Production Expected to Begin in Jul...   \n",
       "11      7131.0  Cramer: NASDAQ Rebound Makes Sense As It Bette...   \n",
       "12      7132.0  Google Maps To Offer Relevant Local COVID-19 I...   \n",
       "13      7133.0  IBM Discontinues Facial Recognition Technology...   \n",
       "14      7134.0  Munster Says Under-The-Radar Apple TV+ Will Co...   \n",
       "15      7135.0  Airlines Soar Again In Pre-Market Trading Amid...   \n",
       "16      7136.0  Apple Gets Patent For Socially-Distanced Group...   \n",
       "17      7137.0  Apple's Top Supplier Foxconn Launched New Recr...   \n",
       "18      7138.0  Apple Preparing Monthly iPad, Mac Payment Plan...   \n",
       "19      7139.0  ESPAÑOL • Cannabis en Argentina, Blockchain Ur...   \n",
       "20      7140.0     PreMarket Prep Stock Of The Day: Genius Brands   \n",
       "21      7141.0  Airlines, Casino Stocks Getting A Lift In Pre-...   \n",
       "22      7142.0            Stocks That Hit 52-Week Highs On Friday   \n",
       "23      7143.0  No Surprise: Guess What Market Is Leading Post...   \n",
       "24      7144.0  Wedbush Maintains Outperform on Apple, Raises ...   \n",
       "25      7145.0  Credit Suisse Maintains Neutral on Apple, Rais...   \n",
       "26      7146.0  FaZe Clan Celebrates 10-Year Anniversary With ...   \n",
       "27      7147.0  US Debt Market Raised $22.5B This Week, The Hi...   \n",
       "28      7148.0  Apple $425 Per Share Case 'On The Horizon,' An...   \n",
       "29      7149.0               FAANG Stocks Seem Immune To COVID-19   \n",
       "30      7150.0  'Apple's #iPhone to join Beijing's 12.2 billio...   \n",
       "31      7151.0  Why Genius Brands Is On A Massive Rally, Addin...   \n",
       "32      7152.0  Warner Music CEO On IPO: Music Is The 'Only Gl...   \n",
       "33      7153.0  Morgan Stanley Bullish On Rising App Store Rev...   \n",
       "34      7154.0            A Look Into Apple's Price Over Earnings   \n",
       "35      7155.0  Zoom Initially Slips Despite Strong Earnings, ...   \n",
       "36      7156.0  Morgan Stanley Maintains Overweight on Apple, ...   \n",
       "37      7157.0  Warner Music Set To Go Public Today, In Antici...   \n",
       "38      7158.0  Peloton Makes Its Fitness App Available On App...   \n",
       "39      7159.0  'Apple is tracking iPhones stolen by looters' ...   \n",
       "40      7160.0  MoneyGram Shares Jump 50% As Western Union Rep...   \n",
       "41      7161.0  Hearing Susquehanna Check Suggests Total iPhon...   \n",
       "42      7162.0  Tesla CEO Musk Says Other Three Officers Shoul...   \n",
       "43      7163.0  A Peek Into The Markets: US Stock Futures Up; ...   \n",
       "44      7164.0  Pepper Spray, Books On Racism, 'I Can't Breath...   \n",
       "45      7165.0  Apple Cuts iPhone Prices in China To Push Sale...   \n",
       "46      7166.0  Gun, Security Stocks Trade Higher After Weeken...   \n",
       "47      7167.0          Carter Worth And Mike Khouw's Apple Trade   \n",
       "48      7168.0  Apple Pays Hacker From India $100,000 For Disc...   \n",
       "49      7169.0  Volume Production of New iPhone Models to Begi...   \n",
       "\n",
       "                         date stock neutral Probability positive Probability  \\\n",
       "0   2020-06-10 11:33:00-04:00  AAPL               0.00%              100.00%   \n",
       "1   2020-06-10 08:14:00-04:00  AAPL             100.00%                0.00%   \n",
       "2   2020-06-10 07:53:00-04:00  AAPL             100.00%                0.00%   \n",
       "3   2020-06-10 07:19:00-04:00  AAPL               0.00%              100.00%   \n",
       "4   2020-06-10 06:27:00-04:00  AAPL             100.00%                0.00%   \n",
       "5   2020-06-10 00:52:00-04:00  AAPL               0.02%               99.98%   \n",
       "6   2020-06-09 15:14:00-04:00  AAPL               0.00%              100.00%   \n",
       "7   2020-06-09 13:58:00-04:00  AAPL              99.99%                0.01%   \n",
       "8   2020-06-09 12:41:00-04:00  AAPL               0.00%              100.00%   \n",
       "9   2020-06-09 11:11:00-04:00  AAPL               8.82%               91.16%   \n",
       "10  2020-06-09 10:59:00-04:00  AAPL              99.97%                0.03%   \n",
       "11  2020-06-09 10:31:00-04:00  AAPL               0.00%              100.00%   \n",
       "12  2020-06-09 03:51:00-04:00  AAPL              99.97%                0.03%   \n",
       "13  2020-06-09 00:25:00-04:00  AAPL              90.36%                0.01%   \n",
       "14  2020-06-08 14:16:00-04:00  AAPL              87.63%               12.35%   \n",
       "15  2020-06-08 09:35:00-04:00  AAPL               0.00%               99.99%   \n",
       "16  2020-06-08 04:34:00-04:00  AAPL              91.74%                8.06%   \n",
       "17  2020-06-07 17:03:00-04:00  AAPL              98.84%                1.15%   \n",
       "18  2020-06-06 18:36:00-04:00  AAPL             100.00%                0.00%   \n",
       "19  2020-06-05 17:19:00-04:00  AAPL             100.00%                0.00%   \n",
       "20  2020-06-05 12:24:00-04:00  AAPL              99.99%                0.00%   \n",
       "21  2020-06-05 10:43:00-04:00  AAPL               0.02%               99.98%   \n",
       "22  2020-06-05 10:30:00-04:00  AAPL              99.94%                0.00%   \n",
       "23  2020-06-05 10:08:00-04:00  AAPL              33.81%                7.34%   \n",
       "24  2020-06-05 08:43:00-04:00  AAPL               0.00%              100.00%   \n",
       "25  2020-06-05 08:27:00-04:00  AAPL              73.70%               14.65%   \n",
       "26  2020-06-05 07:02:00-04:00  AAPL              98.92%                1.08%   \n",
       "27  2020-06-05 05:42:00-04:00  AAPL               0.00%              100.00%   \n",
       "28  2020-06-05 00:07:00-04:00  AAPL              99.99%                0.00%   \n",
       "29  2020-06-04 13:47:00-04:00  AAPL              90.14%                8.76%   \n",
       "30  2020-06-04 10:12:00-04:00  AAPL             100.00%                0.00%   \n",
       "31  2020-06-04 01:01:00-04:00  AAPL               6.48%               93.46%   \n",
       "32  2020-06-03 15:47:00-04:00  AAPL              99.99%                0.00%   \n",
       "33  2020-06-03 11:00:00-04:00  AAPL               0.00%              100.00%   \n",
       "34  2020-06-03 10:47:00-04:00  AAPL             100.00%                0.00%   \n",
       "35  2020-06-03 10:00:00-04:00  AAPL               0.06%                0.01%   \n",
       "36  2020-06-03 08:31:00-04:00  AAPL               0.00%              100.00%   \n",
       "37  2020-06-03 06:09:00-04:00  AAPL             100.00%                0.00%   \n",
       "38  2020-06-03 04:57:00-04:00  AAPL              99.99%                0.00%   \n",
       "39  2020-06-02 11:37:00-04:00  AAPL              79.71%                0.47%   \n",
       "40  2020-06-02 07:44:00-04:00  AAPL               0.46%               99.54%   \n",
       "41  2020-06-02 07:41:00-04:00  AAPL               0.00%                0.00%   \n",
       "42  2020-06-02 06:12:00-04:00  AAPL              99.99%                0.00%   \n",
       "43  2020-06-02 06:03:00-04:00  AAPL               6.33%               93.48%   \n",
       "44  2020-06-02 04:18:00-04:00  AAPL              96.11%                0.01%   \n",
       "45  2020-06-02 00:35:00-04:00  AAPL               0.12%                0.57%   \n",
       "46  2020-06-01 10:14:00-04:00  AAPL               7.81%                3.96%   \n",
       "47  2020-06-01 08:20:00-04:00  AAPL              99.99%                0.00%   \n",
       "48  2020-06-01 06:53:00-04:00  AAPL              99.91%                0.01%   \n",
       "49  2020-06-01 05:39:00-04:00  AAPL             100.00%                0.00%   \n",
       "\n",
       "   negative Probability  \n",
       "0                 0.00%  \n",
       "1                 0.00%  \n",
       "2                 0.00%  \n",
       "3                 0.00%  \n",
       "4                 0.00%  \n",
       "5                 0.00%  \n",
       "6                 0.00%  \n",
       "7                 0.00%  \n",
       "8                 0.00%  \n",
       "9                 0.03%  \n",
       "10                0.00%  \n",
       "11                0.00%  \n",
       "12                0.00%  \n",
       "13                9.63%  \n",
       "14                0.02%  \n",
       "15                0.00%  \n",
       "16                0.21%  \n",
       "17                0.01%  \n",
       "18                0.00%  \n",
       "19                0.00%  \n",
       "20                0.01%  \n",
       "21                0.00%  \n",
       "22                0.06%  \n",
       "23               58.85%  \n",
       "24                0.00%  \n",
       "25               11.65%  \n",
       "26                0.00%  \n",
       "27                0.00%  \n",
       "28                0.01%  \n",
       "29                1.10%  \n",
       "30                0.00%  \n",
       "31                0.06%  \n",
       "32                0.01%  \n",
       "33                0.00%  \n",
       "34                0.00%  \n",
       "35               99.93%  \n",
       "36                0.00%  \n",
       "37                0.00%  \n",
       "38                0.01%  \n",
       "39               19.82%  \n",
       "40                0.00%  \n",
       "41              100.00%  \n",
       "42                0.01%  \n",
       "43                0.18%  \n",
       "44                3.88%  \n",
       "45               99.31%  \n",
       "46               88.23%  \n",
       "47                0.01%  \n",
       "48                0.08%  \n",
       "49                0.00%  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
